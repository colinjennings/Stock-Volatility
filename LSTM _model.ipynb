{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LSTM (Long short-term memory) model\n",
   "id": "5cf4af4b3c107158"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:16:03.233780Z",
     "start_time": "2025-04-09T16:16:02.791894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# scrape the S&P 500 from wikipedia\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "response = requests.get(wiki_url)\n",
    "\n",
    "# wrap the text in StringIO object to prevent Pandas \"future warning\"\n",
    "wiki_tables = pd.read_html(StringIO(response.text))\n",
    "\n",
    "# the first table is the relevant one\n",
    "sp500_table = wiki_tables[0]\n",
    "print(\"SP500 table shape:\", sp500_table.shape)"
   ],
   "id": "1917966dc918179",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500 table shape: (503, 8)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:16:03.242314Z",
     "start_time": "2025-04-09T16:16:03.239394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Notice we also get the sector and sub-industry\n",
    "print(sp500_table.head())"
   ],
   "id": "fbf56ae50f7e0596",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol             Security             GICS Sector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "       CIK      Founded  \n",
      "0    66740         1902  \n",
      "1    91142         1916  \n",
      "2     1800         1888  \n",
      "3  1551152  2013 (1888)  \n",
      "4  1467373         1989  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-09T16:16:03.261777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the stock tickers.\n",
    "ticker_list = sp500_table[\"Symbol\"].tolist()\n",
    "# Replace dots with dashes to be compatible with Yahoo Finance API\n",
    "ticker_list = [ticker.replace(\".\", \"-\") for ticker in ticker_list]\n",
    "# For testing, only use 50 tickers:\n",
    "ticker_list = ticker_list[:50]\n",
    "\n",
    "# Define a 5-year window\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "\n",
    "\n",
    "def get_stock_data(ticker, start_date=start_date, end_date=end_date):\n",
    "    \"\"\"Gets the historic data for a stock based on the ticker and a date range.\"\"\"\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "    # Remove the ticker column (the ticker will be found in the filename)\n",
    "    temp_df.columns = temp_df.columns.droplevel(1)\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "# List of dataframes, each being stock data\n",
    "all_stock_data = []\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for ticker in tqdm(ticker_list, desc=\"Downloading stocks...\"):\n",
    "    temp_df = get_stock_data(ticker)\n",
    "    # Check if empty or full of NaN (happens with API rate limits)\n",
    "    if temp_df.empty or temp_df.isna().all().all():\n",
    "        print(f\"Error getting {ticker}\")\n",
    "    else:\n",
    "        all_stock_data.append(temp_df)\n",
    "    # To prevent too many API calls\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Collected {len(all_stock_data)} stocks\")"
   ],
   "id": "c6fe4af83a813067",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading stocks...:  34%|███▍      | 17/50 [00:17<00:34,  1.03s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:08:20.221252Z",
     "start_time": "2025-04-09T16:08:20.123400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the RSI and add it as a column.\n",
    "def compute_rsi(series, window=14):\n",
    "    \"\"\"Computes the Relative Strength Index (RSI) for a given series.\"\"\"\n",
    "    diff_series = series.diff()\n",
    "    # Gains and losses\n",
    "    gains = diff_series.apply(lambda x: x if x > 0 else 0)\n",
    "    losses = diff_series.apply(lambda x: -x if x < 0 else 0)\n",
    "    # Average gain and loss over a sliding window\n",
    "    avg_gain = gains.rolling(window=window, min_periods=window).mean()\n",
    "    avg_loss = losses.rolling(window=window, min_periods=window).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def classify_rsi(rsi):\n",
    "    \"\"\"Classifies an RSI value into Overbought, Oversold or Neutral.\"\"\"\n",
    "    if np.isnan(rsi):\n",
    "        return np.nan\n",
    "    if rsi > 70:\n",
    "        return \"Overbought\"\n",
    "    elif rsi < 30:\n",
    "        return \"Oversold\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "# Compute the RSI and RSI class for all stocks\n",
    "for stock_data, ticker in zip(all_stock_data, ticker_list):\n",
    "    stock_data['RSI'] = compute_rsi(stock_data['Close'])\n",
    "    stock_data['RSI_class'] = stock_data['RSI'].apply(classify_rsi)\n",
    "\n",
    "# Example for one stock (e.g., $MMM if it is first in the list)\n",
    "print(\"Example data for \" + ticker_list[0])\n",
    "print(all_stock_data[0].head(20))"
   ],
   "id": "e07524496a8f34e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data for MMM\n",
      "Price            Close        High         Low        Open    Volume  \\\n",
      "Date                                                                   \n",
      "2020-01-02  122.357414  122.364212  120.413285  120.780359   4307633   \n",
      "2020-01-03  121.303795  121.446543  119.386858  120.331736   2950412   \n",
      "2020-01-06  121.419373  121.480556  119.876308  120.420121   2389608   \n",
      "2020-01-07  120.929947  121.344608  120.195798  121.188251   2598908   \n",
      "2020-01-08  122.785698  123.377098  120.760003  120.997925   3298927   \n",
      "2020-01-09  123.173149  123.438251  122.194292  123.383879   3284575   \n",
      "2020-01-10  122.676903  123.839296  122.452577  123.451828   2516145   \n",
      "2020-01-13  122.982819  123.873312  122.602148  122.996415   2500597   \n",
      "2020-01-14  123.288734  124.090852  122.500213  122.982843   3481795   \n",
      "2020-01-15  122.207893  123.621806  121.800043  123.322713   2968711   \n",
      "2020-01-16  123.043999  123.173151  122.146707  122.819673   2571280   \n",
      "2020-01-17  123.275101  123.818914  122.833259  123.037191   4413360   \n",
      "2020-01-21  121.942780  123.655785  121.426159  123.614996   4027769   \n",
      "2020-01-22  121.018280  122.744882  120.916320  122.024342   2894559   \n",
      "2020-01-23  120.821182  121.297014  119.903496  120.365732   2820646   \n",
      "2020-01-24  121.099876  122.228280  120.678429  121.331001   3161148   \n",
      "2020-01-27  119.386856  122.758482  119.325673  122.262255   5216234   \n",
      "2020-01-28  112.555244  117.150448  112.194981  116.851355  10669157   \n",
      "2020-01-29  110.121681  113.160223  109.802194  113.078646   6391304   \n",
      "2020-01-30  110.468369  110.556732  107.919255  109.441925   5211092   \n",
      "\n",
      "Price             RSI RSI_class  \n",
      "Date                             \n",
      "2020-01-02        NaN       NaN  \n",
      "2020-01-03        NaN       NaN  \n",
      "2020-01-06        NaN       NaN  \n",
      "2020-01-07        NaN       NaN  \n",
      "2020-01-08        NaN       NaN  \n",
      "2020-01-09        NaN       NaN  \n",
      "2020-01-10        NaN       NaN  \n",
      "2020-01-13        NaN       NaN  \n",
      "2020-01-14        NaN       NaN  \n",
      "2020-01-15        NaN       NaN  \n",
      "2020-01-16        NaN       NaN  \n",
      "2020-01-17        NaN       NaN  \n",
      "2020-01-21        NaN       NaN  \n",
      "2020-01-22  42.888122   Neutral  \n",
      "2020-01-23  42.008672   Neutral  \n",
      "2020-01-24  48.846216   Neutral  \n",
      "2020-01-27  40.260489   Neutral  \n",
      "2020-01-28  25.040485  Oversold  \n",
      "2020-01-29  13.513497  Oversold  \n",
      "2020-01-30  13.309870  Oversold  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:08:20.340869Z",
     "start_time": "2025-04-09T16:08:20.308591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Implement logarithmic returns and volatility as new columns.\n",
    "def compute_log_return(series):\n",
    "    \"\"\"Computes the logarithmic return of a series.\"\"\"\n",
    "    day_offset = series.shift(1)\n",
    "    return np.log(series / day_offset)\n",
    "\n",
    "\n",
    "def compute_volatility(series, window=14):\n",
    "    \"\"\"Computes the volatility of a logarithmic return series using a rolling window.\"\"\"\n",
    "    volatility = series.rolling(window=window, min_periods=window).std()\n",
    "    return volatility\n",
    "\n",
    "\n",
    "# Compute log returns and volatility for all stocks\n",
    "for stock_data, ticker in zip(all_stock_data, ticker_list):\n",
    "    stock_data['Log_return'] = compute_log_return(stock_data['Close'])\n",
    "    stock_data['Volatility'] = compute_volatility(stock_data['Log_return'])\n",
    "\n",
    "print(\"Example data for \" + ticker_list[0])\n",
    "print(all_stock_data[0].head(20))"
   ],
   "id": "d57139b4649ec32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data for MMM\n",
      "Price            Close        High         Low        Open    Volume  \\\n",
      "Date                                                                   \n",
      "2020-01-02  122.357414  122.364212  120.413285  120.780359   4307633   \n",
      "2020-01-03  121.303795  121.446543  119.386858  120.331736   2950412   \n",
      "2020-01-06  121.419373  121.480556  119.876308  120.420121   2389608   \n",
      "2020-01-07  120.929947  121.344608  120.195798  121.188251   2598908   \n",
      "2020-01-08  122.785698  123.377098  120.760003  120.997925   3298927   \n",
      "2020-01-09  123.173149  123.438251  122.194292  123.383879   3284575   \n",
      "2020-01-10  122.676903  123.839296  122.452577  123.451828   2516145   \n",
      "2020-01-13  122.982819  123.873312  122.602148  122.996415   2500597   \n",
      "2020-01-14  123.288734  124.090852  122.500213  122.982843   3481795   \n",
      "2020-01-15  122.207893  123.621806  121.800043  123.322713   2968711   \n",
      "2020-01-16  123.043999  123.173151  122.146707  122.819673   2571280   \n",
      "2020-01-17  123.275101  123.818914  122.833259  123.037191   4413360   \n",
      "2020-01-21  121.942780  123.655785  121.426159  123.614996   4027769   \n",
      "2020-01-22  121.018280  122.744882  120.916320  122.024342   2894559   \n",
      "2020-01-23  120.821182  121.297014  119.903496  120.365732   2820646   \n",
      "2020-01-24  121.099876  122.228280  120.678429  121.331001   3161148   \n",
      "2020-01-27  119.386856  122.758482  119.325673  122.262255   5216234   \n",
      "2020-01-28  112.555244  117.150448  112.194981  116.851355  10669157   \n",
      "2020-01-29  110.121681  113.160223  109.802194  113.078646   6391304   \n",
      "2020-01-30  110.468369  110.556732  107.919255  109.441925   5211092   \n",
      "\n",
      "Price             RSI RSI_class  Log_return  Volatility  \n",
      "Date                                                     \n",
      "2020-01-02        NaN       NaN         NaN         NaN  \n",
      "2020-01-03        NaN       NaN   -0.008648         NaN  \n",
      "2020-01-06        NaN       NaN    0.000952         NaN  \n",
      "2020-01-07        NaN       NaN   -0.004039         NaN  \n",
      "2020-01-08        NaN       NaN    0.015229         NaN  \n",
      "2020-01-09        NaN       NaN    0.003151         NaN  \n",
      "2020-01-10        NaN       NaN   -0.004037         NaN  \n",
      "2020-01-13        NaN       NaN    0.002491         NaN  \n",
      "2020-01-14        NaN       NaN    0.002484         NaN  \n",
      "2020-01-15        NaN       NaN   -0.008805         NaN  \n",
      "2020-01-16        NaN       NaN    0.006818         NaN  \n",
      "2020-01-17        NaN       NaN    0.001876         NaN  \n",
      "2020-01-21        NaN       NaN   -0.010867         NaN  \n",
      "2020-01-22  42.888122   Neutral   -0.007610         NaN  \n",
      "2020-01-23  42.008672   Neutral   -0.001630    0.007111  \n",
      "2020-01-24  48.846216   Neutral    0.002304    0.006788  \n",
      "2020-01-27  40.260489   Neutral   -0.014247    0.007751  \n",
      "2020-01-28  25.040485  Oversold   -0.058925    0.017297  \n",
      "2020-01-29  13.513497  Oversold   -0.021858    0.016771  \n",
      "2020-01-30  13.309870  Oversold    0.003143    0.016771  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Earnings Data and Visualization\n",
    "In\n",
    "the\n",
    "next\n",
    "section, earnings\n",
    "data is added and a\n",
    "couple\n",
    "of\n",
    "visualizations\n",
    "are\n",
    "produced."
   ],
   "id": "6060e245a1f4f9ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your actual Alpha Vantage API key\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "ticker = \"AAPL\"\n",
    "\n",
    "url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Example: Print the first few quarterly earnings reports\n",
    "earnings_history = data.get(\"quarterlyEarnings\", [])\n",
    "print([report for report in earnings_history[:5]])"
   ],
   "id": "21b12ce26b1e66b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_earnings_data(ticker):\n",
    "    \"\"\"Gets the earnings data for a ticker from Alpha Vantage.\"\"\"\n",
    "    url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if \"quarterlyEarnings\" not in data:\n",
    "        return None\n",
    "    earnings_df = pd.DataFrame(data['quarterlyEarnings'])\n",
    "    earnings_df['reportedDate'] = pd.to_datetime(earnings_df['reportedDate'])\n",
    "    earnings_df['reportedEPS'] = pd.to_numeric(earnings_df['reportedEPS'])\n",
    "    return earnings_df\n",
    "\n",
    "\n",
    "def add_earnings_info(stock_df, earnings_df):\n",
    "    \"\"\"Adds earnings info (days to next earnings and last reported EPS) to the stock dataframe.\"\"\"\n",
    "    if earnings_df is None:\n",
    "        print(\"Error: Earnings data is None. Check retrieval process.\")\n",
    "        return None\n",
    "\n",
    "    stock_df['Days_to_Earnings'] = np.nan\n",
    "    stock_df['Last_EPS'] = np.nan\n",
    "    earnings_dates = earnings_df['reportedDate'].sort_values().tolist()\n",
    "\n",
    "    for current_date, row in stock_df.iterrows():\n",
    "        future_earnings = [date for date in earnings_dates if date >= current_date]\n",
    "        past_earnings = [date for date in earnings_dates if date < current_date]\n",
    "        if future_earnings:\n",
    "            next_earnings_date = future_earnings[0]\n",
    "            days_to_earnings = (next_earnings_date - current_date).days\n",
    "            stock_df.at[current_date, 'Days_to_Earnings'] = days_to_earnings\n",
    "        if past_earnings:\n",
    "            last_earnings_row = earnings_df.loc[earnings_df['reportedDate'] == past_earnings[-1]]\n",
    "            last_eps = last_earnings_row['reportedEPS'].values[0]\n",
    "            stock_df.at[current_date, 'Last_EPS'] = last_eps\n",
    "\n",
    "\n",
    "# Example: add earnings info for the first stock\n",
    "add_earnings_info(all_stock_data[0], get_earnings_data(ticker_list[0]))\n",
    "print(all_stock_data[0][10:25])"
   ],
   "id": "6049ccf63834d851"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run earnings data addition for every company (using tqdm to track progress)\n",
    "for stock_data, ticker in tqdm(zip(all_stock_data, ticker_list), total=len(all_stock_data)):\n",
    "    add_earnings_info(stock_data, get_earnings_data(ticker))\n",
    "    time.sleep(1)"
   ],
   "id": "81184e2641894012"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test earnings info with the last stock\n",
    "print(f\"Data for {ticker_list[49]}:\")\n",
    "print(all_stock_data[49])"
   ],
   "id": "4f667cce9b22f3b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualization of Earnings vs. Volatility\n",
    "Below, an\n",
    "example\n",
    "plot is created\n",
    "to\n",
    "compare\n",
    "how\n",
    "the\n",
    "number\n",
    "of\n",
    "days\n",
    "to\n",
    "the\n",
    "next\n",
    "earnings\n",
    "report\n",
    "relates\n",
    "to\n",
    "logarithmic\n",
    "volatility."
   ],
   "id": "25df1c632a04c80e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'all_stock_data' is a list of pandas DataFrames and 'ticker_list' is a list of ticker names\n",
    "stock_data = all_stock_data[23]\n",
    "\n",
    "# Print out the list of available columns for debugging\n",
    "print(\"Available columns in stock_data:\", stock_data.columns.tolist())\n",
    "\n",
    "# Check if both required columns exist\n",
    "required_columns = ['Days_to_Earnings', 'Volatility']\n",
    "missing_columns = [col for col in required_columns if col not in stock_data.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(\n",
    "        f\"Error: The following columns are missing from the DataFrame: {missing_columns}. Please verify your data source.\")\n",
    "else:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(stock_data['Days_to_Earnings'], stock_data['Volatility'])\n",
    "    plt.title(f\"Days to Earnings vs Logarithmic Volatility for ${ticker_list[23]}\")\n",
    "    plt.xlabel('Days to Earnings')\n",
    "    plt.ylabel('Volatility')\n",
    "    plt.show()"
   ],
   "id": "33cc6d9db7a989aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Additional visualization: Rolling Volume and Rolling Volatility for an example stock\n",
    "stock_data = all_stock_data[12]\n",
    "rolling_volume = stock_data['Volume'].rolling(window=10).mean()\n",
    "rolling_volatility = stock_data['Volatility'].rolling(window=10).mean()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax1.plot(stock_data.index, rolling_volume, color='blue', alpha=0.6)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Rolling Volume', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(stock_data.index, rolling_volatility, color='red', alpha=0.6)\n",
    "ax2.set_ylabel('Rolling Volatility', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title(f\"Rolling Volume and Volatility for ${ticker_list[12]}\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "77304da1371d2061"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM Model for Predicting Volatility\n",
    "In\n",
    "the\n",
    "next\n",
    "part, we\n",
    "build\n",
    "an\n",
    "LSTM\n",
    "model\n",
    "to\n",
    "predict\n",
    "the\n",
    "volatility\n",
    "for one of the stocks based on its historical volatility series."
   ],
   "id": "cdeb940243dcd37a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:09:30.600091Z",
     "start_time": "2025-04-09T16:09:17.711134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Select one stock’s volatility data for prediction;\n",
    "# Here we use the first stock in our all_stock_data list. Ensure there are no NaN values.\n",
    "stock_data = all_stock_data[0].copy()\n",
    "df = stock_data[['Volatility']].dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the volatility data to the [0,1] range.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "# Define a function to create sequences from the data.\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Set a window size; here we use the past 10 days to predict the next day’s volatility.\n",
    "window_size = 10\n",
    "X, y = create_sequences(scaled_data, window_size)\n",
    "\n",
    "# Split the sequences into training and test sets (80% training, 20% test)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")"
   ],
   "id": "c9e69f578c79a916",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 987, Test samples: 247\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:11:36.960364Z",
     "start_time": "2025-04-09T16:11:36.918963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the LSTM model.\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='tanh', return_sequences=True, input_shape=(window_size, 1)),\n",
    "    LSTM(25, activation='tanh'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ],
   "id": "6a758772cb9516e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anish/STUDY/Spring 25/F. Data Science/Stock-Volatility/.venv1/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m50\u001B[0m)         │        \u001B[38;5;34m11,200\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m25\u001B[0m)             │         \u001B[38;5;34m7,600\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m26\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m18,826\u001B[0m (73.54 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,826</span> (73.54 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m18,826\u001B[0m (73.54 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,826</span> (73.54 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:14:50.165746Z",
     "start_time": "2025-04-09T16:14:49.350236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up early stopping to prevent overfitting.\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[es],\n",
    "                    verbose=1)"
   ],
   "id": "28a32721c12e36a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m es = EarlyStopping(monitor=\u001B[33m'\u001B[39m\u001B[33mval_loss\u001B[39m\u001B[33m'\u001B[39m, patience=\u001B[32m10\u001B[39m, restore_best_weights=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Train the model.\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m history = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mes\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m                    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/STUDY/Spring 25/F. Data Science/Stock-Volatility/.venv1/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/STUDY/Spring 25/F. Data Science/Stock-Volatility/.venv1/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:132\u001B[39m, in \u001B[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001B[39m\u001B[34m(iterator)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;129m@tf\u001B[39m.autograph.experimental.do_not_convert\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmulti_step_on_iterator\u001B[39m(iterator):\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.steps_per_execution == \u001B[32m1\u001B[39m:\n\u001B[32m    131\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m tf.experimental.Optional.from_value(\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m             \u001B[43mone_step_on_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_next\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m         )\n\u001B[32m    135\u001B[39m     \u001B[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001B[39;00m\n\u001B[32m    136\u001B[39m     empty_outputs = tf.experimental.Optional.empty(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[31mValueError\u001B[39m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model on the test set.\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss (MSE):\", test_loss)"
   ],
   "id": "b0c7bf5c7aa4088d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make predictions on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse-transform the scaled predictions and true values.\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Plot the true volatility versus the predicted volatility.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_inv, label='True Volatility', color='blue')\n",
    "plt.plot(y_pred_inv, label='Predicted Volatility', color='red')\n",
    "plt.title(\"LSTM Prediction: True vs Predicted Volatility\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "371556ee9b0c613e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Plotting Training and Validation Loss History\n",
    "# ------------------------------------------------------\n",
    "def plot_loss_history(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', linewidth=2)\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\", fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=12)\n",
    "    plt.ylabel(\"Loss (MSE)\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function with your history object\n",
    "plot_loss_history(history)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Plotting True vs Predicted Volatility Over Time\n",
    "# ------------------------------------------------------\n",
    "def plot_true_vs_pred(y_true, y_pred):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_true, label='True Volatility', color='blue', linewidth=2)\n",
    "    plt.plot(y_pred, label='Predicted Volatility', color='red', linewidth=2)\n",
    "    plt.title(\"True vs. Predicted Volatility\", fontsize=14)\n",
    "    plt.xlabel(\"Time Step\", fontsize=12)\n",
    "    plt.ylabel(\"Volatility\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use your inverse-transformed values\n",
    "plot_true_vs_pred(y_test_inv, y_pred_inv)\n",
    "\n",
    "scatter_true_vs_pred(y_test_inv, y_pred_inv)\n",
    "\n",
    "# 4. Plotting Residual Errors Over Time\n",
    "def plot_residuals_over_time(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(residuals, label='Residuals', color='green', linewidth=2)\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    plt.title(\"Residual Errors Over Time\", fontsize=14)\n",
    "    plt.xlabel(\"Time Step\", fontsize=12)\n",
    "    plt.ylabel(\"Residual Error\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function using your inverse-transformed true and predicted values\n",
    "plot_residuals_over_time(y_test_inv, y_pred_inv)\n",
    "\n",
    "\n",
    "# 5. Histogram of Residuals\n",
    "def plot_residuals_histogram(y_true, y_pred, bins=30):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=bins, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    plt.title(\"Histogram of Residuals\", fontsize=14)\n",
    "    plt.xlabel(\"Residual Error\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_residuals_histogram(y_test_inv, y_pred_inv)\n",
    "\n",
    "\n",
    "# 6. Q-Q Plot of Residuals\n",
    "def plot_qq_residuals(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\", fontsize=14)\n",
    "    plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n",
    "    plt.ylabel(\"Sample Quantiles\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_qq_residuals(y_test_inv, y_pred_inv)\n",
    "\n",
    "# 7. Autocorrelation Plot for Residuals\n",
    "def plot_residuals_autocorrelation(y_true, y_pred, lags=40):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(residuals, lags=lags, alpha=0.05)\n",
    "    plt.title(\"Autocorrelation of Residuals\", fontsize=14)\n",
    "    plt.xlabel(\"Lag\", fontsize=12)\n",
    "    plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_residuals_autocorrelation(y_test_inv, y_pred_inv)\n",
    "\n",
    "\n",
    "# 8. Rolling Statistics (Mean & Std) for Residuals\n",
    "def plot_rolling_stats(y_true, y_pred, window=50):\n",
    "    residuals = pd.Series(y_true - y_pred)\n",
    "    rolling_mean = residuals.rolling(window=window).mean()\n",
    "    rolling_std = residuals.rolling(window=window).std()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(residuals, label=\"Residuals\", color='gray', alpha=0.5)\n",
    "    plt.plot(rolling_mean, label=f\"Rolling Mean (window={window})\", color='blue', linewidth=2)\n",
    "    plt.plot(rolling_std, label=f\"Rolling Std Dev (window={window})\", color='red', linewidth=2)\n",
    "    plt.title(\"Rolling Mean and Standard Deviation of Residuals\", fontsize=14)\n",
    "    plt.xlabel(\"Time Step\", fontsize=12)\n",
    "    plt.ylabel(\"Error Value\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_rolling_stats(y_test_inv, y_pred_inv, window=50)\n",
    "\n",
    "\n",
    "# 9. Density Plot (KDE) for Prediction Errors\n",
    "def plot_kde_errors(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(residuals, shade=True, color=\"purple\")\n",
    "    plt.title(\"Density Plot (KDE) of Prediction Errors\", fontsize=14)\n",
    "    plt.xlabel(\"Residual Error\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_kde_errors(y_test_inv, y_pred_inv)\n",
    "\n"
   ],
   "id": "bf924a785ef53ec8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "aa2d0362ac955815",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "9209455069961d73",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
